{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- all titles with `**` & all orange comments are to be discussed AFTER first build. discussed when \"how to make model better\"\n",
    "- here, all imports are added when needed. during interview, add them when needed but at the top of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x_0  x_1  x_2  y\n",
      "0  1.0    0    0  0\n",
      "1  0.0    0    5  0\n",
      "2  1.0    1    3  1\n",
      "3  0.0    1    1  0\n",
      "4  0.0    1    1  1\n",
      "5  0.0    1    2  0\n",
      "6  1.0    0    1  1\n",
      "7  1.1    0    1  0\n",
      "8  1.0    0    0  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = 'data.csv'\n",
    "data = pd.read_csv(data_path)       # dataframe\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features, data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 4)\n"
     ]
    }
   ],
   "source": [
    "# FEATURES: measurable properties/attributes we can use to predict\n",
    "# check number of data points (rows) and number of features (columns except target 'y' in this case)\n",
    "\n",
    "print(data.shape)       # tuple (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 9 rows\n",
    "- 4 columns \n",
    "    - 3 features (since 'y' is a column too in this case)\n",
    "    - 3 x 9 : 27 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range of features\n",
    "\n",
    "By knowing the range of each feature, we can apply proper normalization (transform feature values to a standard scale) to ensure all features contribute proportionaly during training.\n",
    "- For ex., if the range of one feature is 1000x larger that another, then during loss minimization, the gradients associated with the larger-scaled feature will likely be larger. This disproportion can cause the optimization process to overemphasize that feature, even though that feature might not actually be too influential in the prediction itself, potentially skewing weight updates and adversely affecting the overall training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of features: \n",
      "('x_0', 1.1)\n",
      "('x_1', 1.0)\n",
      "('x_2', 5.0)\n"
     ]
    }
   ],
   "source": [
    "# determine range of each feature:      max - min\n",
    "\n",
    "features_columns = [col for col in data if col != 'y']\n",
    "features_ranges = {}\n",
    "\n",
    "for feature in features_columns:\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    features_ranges[feature] = float(max_val - min_val)\n",
    "\n",
    "print(\"Range of features: \")\n",
    "for range in features_ranges.items():\n",
    "    print(range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and package selection\n",
    "\n",
    "- Because the target column consists of 0s and 1s, this is a binary classification problem (predicting y from x features)\n",
    "    - Use a multi-layer perceptron (MLP)\n",
    "\n",
    "- Use pytorch to define, train and evaluate the model\n",
    "- Use scikit-learn to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# seperate features and target\n",
    "features_values = data[features_columns].values\n",
    "target_values = data['y'].values\n",
    "\n",
    "# convert to tensors    (tensors: multidimensional homogenous data structures, good for parallelism and have many operation optimizations in packages like pytorch)\n",
    "features_values = torch.Tensor(features_values)\n",
    "target_values = torch.Tensor(target_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `**` Normalization / scale data `**`\n",
    "\n",
    "StandardScaler standardizes features by rescale them to have a mean of 0 and a standard deviation of 1.\n",
    "- Standardization does NOT change the shape of the data: it does NOT transform the data into a Gaussian distribution, it only standardizes the scale. The underlying distribution of the data remains unchanged\n",
    "    - i.e. DISTRIBUTION of the original data remains the same, but the numerical values are scaled such that 0 is the center/average and each data point is spread out by 1 unit\n",
    "\n",
    "<br><br>\n",
    "$x' = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $x$ = original data point  \n",
    "- $\\mu$ = mean of the feature (before standardization)  \n",
    "- $\\sigma$ = standard deviation of the feature (before standardization)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import StandardScaler\\n\\nscaler = StandardScaler()\\nfeatures_values = scaler.fit_transform(features_values)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_values = scaler.fit_transform(features_values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data (80% train, 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_values, target_values, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pytorch Dataset and Dataloader\n",
    "\n",
    "- Dataset: stores the samples and their labels\n",
    "- DataLoader: wraps an iterable around the `Dataset` to enable easy access to the samples. Makes it parallelized to load in batches to the model\n",
    "\n",
    "<br>\n",
    "\n",
    "- Batch: a subset of the training data processed together in one forward/backward pass\n",
    "    - batch size value depends on memory constraints, model size, dataset size, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n**ADD SHUFFLE**\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create datasets\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\"\"\"\n",
    "**ADD SHUFFLE**\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "<i>Note: Layer sizes decrease gradually to funnel information</i>\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "- Input layer: (# features) neurons\n",
    "<br><br>\n",
    "- Hidden layer 1: 64 neurons (2-3x input features)\n",
    "<br><br>\n",
    "- Hidden layer 2: 32 neurons (half previous layer for gradual dimension reduction)\n",
    "<br><br>\n",
    "- Output layer: 1 neuron (for binary classification)\n",
    "<br><br>\n",
    "- Activation function: **ReLU**. max(0, x) - returns x if positive, 0 if negative\n",
    "    - prevents vanishing gradient problem (when gradients used to update the network become very slow. so network learns too slow or not at all)\n",
    "<br><br>\n",
    "- Sigmoid: squash output between 0 and 1 (for binary classification problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):         # nn.Module is the base class for all neural networks. Our model will be a subclass that inherits this superclass\n",
    "    def __init__(self, input_size):     # input_size: number of the features, `len(features_columns)`\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),       \n",
    "            nn.ReLU(),                                  \n",
    "            # **  IMPROVEMENT: nn.Dropout(0.2),\n",
    "            # **  IMPROVEMENT: nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "# initialize\n",
    "model = NeuralNetwork(len(features_columns))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Measure how inaccurate model predictions are and give gradient direction for optimization. The model **learns by MINIMIZING the loss function** (i.e. minimizing its errors).\n",
    "\n",
    "<br><br>\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $n$ = number of samples  \n",
    "- $y_i$ = actual (true) value  \n",
    "- $\\hat{y}_i$ = predicted value  \n",
    "  \n",
    "*scaled by* $\\frac{1}{n}$ *so the derivative is cleaner for backpropagation*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n# For binary classification, pytorch's BCE:\\nloss = nn.BCELoss\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.MSELoss\n",
    "\n",
    "\"\"\" \n",
    "# For binary classification, pytorch's BCE:\n",
    "loss = nn.BCELoss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to iteratively adjust parameters in order to minimize the loss function. \n",
    "- Computes the gradient (partial derivatives) of the loss function w.r.t the parameters\n",
    "- Updates parameters in the direction of steepest descent (negative gradient)\n",
    "\n",
    "<br><br>\n",
    "Learning rate (lr): scaling factor that controls how much the model updates the weights at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "- 1 Epoch: 1 complete pass through the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
