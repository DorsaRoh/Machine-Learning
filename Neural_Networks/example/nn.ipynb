{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x_0  x_1  x_2  y\n",
      "0  1.00    0    0  0\n",
      "1  0.00    0    5  0\n",
      "2  1.00    1    3  1\n",
      "3  0.00    1    1  0\n",
      "4  0.00    1    1  1\n",
      "5  0.00    1    1  0\n",
      "6  3.71    0    1  1\n",
      "7  1.10    0    1  0\n",
      "8  1.00    0    0  1\n",
      "9  1.00    1    1  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load csv file using pandas\n",
    "data_path = 'data.csv'\n",
    "data = pd.read_csv(data_path)           # data is a dataframe/2d tabular representation\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# check number of data points (rows) and number of features (columns except target 'y')\n",
    "\n",
    "print(data.shape)  # returns tuple (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of rows: 10\n",
    "- Number of columns: 4\n",
    "<br><br>\n",
    "- Number of features: 3 (since 'y' is a column in this case)\n",
    "- Number of data points: rows x features \n",
    "    - 3 x 10 : 30 data points\n",
    "<br><br>\n",
    "\n",
    "Features: measurable properties/attributes we can use to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of features\n",
    "\n",
    "- By knowing the range of each feature, we can apply proper normalization (e.g., min-max scaling or standardization) to ensure all features contribute proportionately during training\n",
    "    - For ex., if the range of one feature is 10 times larger than that of another, then during loss minimization, the gradients associated with the larger-scaled feature will likely be larger. This disproportion can cause the optimization process to overemphasize that feature, even though that feature might not actually be too influential in the prediction, potentially skewing weight updates and adversely affecting the overall training process\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of features: \n",
      "('x_0', 3.71)\n",
      "('x_1', 1.0)\n",
      "('x_2', 5.0)\n"
     ]
    }
   ],
   "source": [
    "# determine range of each feature\n",
    "\n",
    "features_columns = [col for col in data if col != 'y']\n",
    "feature_ranges = {}\n",
    "for feature in features_columns:\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    feature_ranges[feature] = float(max_val - min_val)\n",
    "\n",
    "print(\"range of features: \")\n",
    "for feature in feature_ranges.items():\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and package selection\n",
    "\n",
    "- Because the target column consists of 0s and 1s, this is likely a binary classification problem (predicting y from x features)\n",
    "    - Use a feedforward neural network\n",
    "- Use  pytorch for defining the model, training, evals\n",
    "- Use the scikit-learn package to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# prepare the data for pytorch\n",
    "# -------------------------\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# separate features and target, converting them to numpy arrays with type float32\n",
    "features_values = data[features_columns].values\n",
    "target_values = data['y'].values\n",
    "\n",
    "# convert the numpy arrays to pytorch tensors\n",
    "tensor_features = torch.Tensor(features_values)\n",
    "tensor_target = torch.Tensor(target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data (80% train, 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42) \n",
    "\n",
    "tensor_x_train = torch.Tensor(x_train)\n",
    "tensor_x_test = torch.Tensor(x_test)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "tensor_y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pytorch datasets and dataloaders for the train and test sets\n",
    "\n",
    "TODO:\n",
    "- Why? also know exactly what that code is doing\n",
    "- what is batch size? why 2?\n",
    "- why set shuffle to true for train and false for test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)        \n",
    "test_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network\n",
    "\n",
    "TODO:\n",
    "- how to determine number of hidden layers? and number of neurons they each take? \n",
    "- why relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of features = number of input neurons\n",
    "\n",
    "        # Input layer (3 features) \n",
    "            # -> Hidden layer1 (10 neurons) \n",
    "            # -> Hidden layer2 (5 neurons) \n",
    "            # -> output (2 neurons, 1 output value)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_features, 10),   # first hidden layer with 10 neurons\n",
    "            nn.ReLU(),                       # activation function RELU: max(0, x) \n",
    "            nn.Linear(10, 5),                # second hidden layer with 5 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),                 # output layer\n",
    "            nn.Sigmoid()                     # final activation for binary classification (probabilities between 0 and 1)\n",
    "        )  \n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why BCEloss?\n",
    "- why adam optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# intialize the model, loss function, and optimizer\n",
    "\n",
    "input_dim = len(features_columns)\n",
    "model = NeuralNetwork(input_dim)\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train method to train the model & print training accuracy\n",
    "\n",
    "- what is epoch? how to determine its value?\n",
    "- what is batch?\n",
    "- what is .zero_grad()?\n",
    "- determine what EACH line of code is doing in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Average Loss: 0.3010\n",
      "Epoch 2/50, Average Loss: 0.2996\n",
      "Epoch 3/50, Average Loss: 0.2967\n",
      "Epoch 4/50, Average Loss: 0.2916\n",
      "Epoch 5/50, Average Loss: 0.2895\n",
      "Epoch 6/50, Average Loss: 0.2865\n",
      "Epoch 7/50, Average Loss: 0.2847\n",
      "Epoch 8/50, Average Loss: 0.2831\n",
      "Epoch 9/50, Average Loss: 0.2809\n",
      "Epoch 10/50, Average Loss: 0.2773\n",
      "Epoch 11/50, Average Loss: 0.2769\n",
      "Epoch 12/50, Average Loss: 0.2753\n",
      "Epoch 13/50, Average Loss: 0.2750\n",
      "Epoch 14/50, Average Loss: 0.2729\n",
      "Epoch 15/50, Average Loss: 0.2707\n",
      "Epoch 16/50, Average Loss: 0.2733\n",
      "Epoch 17/50, Average Loss: 0.2690\n",
      "Epoch 18/50, Average Loss: 0.2665\n",
      "Epoch 19/50, Average Loss: 0.2657\n",
      "Epoch 20/50, Average Loss: 0.2639\n",
      "Epoch 21/50, Average Loss: 0.2648\n",
      "Epoch 22/50, Average Loss: 0.2634\n",
      "Epoch 23/50, Average Loss: 0.2610\n",
      "Epoch 24/50, Average Loss: 0.2608\n",
      "Epoch 25/50, Average Loss: 0.2616\n",
      "Epoch 26/50, Average Loss: 0.2591\n",
      "Epoch 27/50, Average Loss: 0.2609\n",
      "Epoch 28/50, Average Loss: 0.2584\n",
      "Epoch 29/50, Average Loss: 0.2571\n",
      "Epoch 30/50, Average Loss: 0.2567\n",
      "Epoch 31/50, Average Loss: 0.2565\n",
      "Epoch 32/50, Average Loss: 0.2573\n",
      "Epoch 33/50, Average Loss: 0.2541\n",
      "Epoch 34/50, Average Loss: 0.2546\n",
      "Epoch 35/50, Average Loss: 0.2527\n",
      "Epoch 36/50, Average Loss: 0.2550\n",
      "Epoch 37/50, Average Loss: 0.2539\n",
      "Epoch 38/50, Average Loss: 0.2508\n",
      "Epoch 39/50, Average Loss: 0.2524\n",
      "Epoch 40/50, Average Loss: 0.2509\n",
      "Epoch 41/50, Average Loss: 0.2513\n",
      "Epoch 42/50, Average Loss: 0.2512\n",
      "Epoch 43/50, Average Loss: 0.2504\n",
      "Epoch 44/50, Average Loss: 0.2506\n",
      "Epoch 45/50, Average Loss: 0.2491\n",
      "Epoch 46/50, Average Loss: 0.2491\n",
      "Epoch 47/50, Average Loss: 0.2493\n",
      "Epoch 48/50, Average Loss: 0.2499\n",
      "Epoch 49/50, Average Loss: 0.2477\n",
      "Epoch 50/50, Average Loss: 0.2488\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, loss_criterion, optimizer, num_epochs=50):\n",
    "    model.train()   # set to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()                           # reset gradients\n",
    "            y_pred = model(batch_features)                  # forward pass using batch data\n",
    "            loss = loss_criterion(y_pred.squeeze(), batch_labels)  # compute loss using batch data\n",
    "            loss.backward()                                 # backpropagation\n",
    "            optimizer.step()                                # update weights\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_model(model, train_dataloader, loss_criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can this be done better? That improves accuracy? (add those changes as comments)\n",
    "    - bigger scale/more layers/neurons\n",
    "    - dropout... (LEARN WHAT THIS IS)\n",
    "    - \n",
    "\n",
    "- If this was time series how would you take that into account?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
