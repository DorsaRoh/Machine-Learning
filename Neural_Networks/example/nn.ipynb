{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x_0  x_1  x_2  y\n",
      "0  1.00    0    0  0\n",
      "1  0.00    0    5  0\n",
      "2  1.00    1    3  1\n",
      "3  0.00    1    1  0\n",
      "4  0.00    1    1  1\n",
      "5  0.00    1    1  0\n",
      "6  3.71    0    1  1\n",
      "7  1.10    0    1  0\n",
      "8  1.00    0    0  1\n",
      "9  1.00    1    1  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load csv file using pandas\n",
    "data_path = 'data.csv'\n",
    "data = pd.read_csv(data_path)           # data is a dataframe/2d tabular representation\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "# check number of data points (rows) and number of features (columns except target 'y')\n",
    "\n",
    "print(data.shape)  # returns tuple (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of rows: 10\n",
    "- Number of columns: 4\n",
    "<br><br>\n",
    "- Number of features: 3 (since 'y' is a column in this case)\n",
    "- Number of data points: rows x features \n",
    "    - 3 x 10 : 30 data points\n",
    "<br><br>\n",
    "\n",
    "Features: measurable properties/attributes we can use to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of features\n",
    "\n",
    "- By knowing the range of each feature, we can apply proper normalization (e.g., min-max scaling or standardization) to ensure all features contribute proportionately during training\n",
    "    - For ex., if the range of one feature is 10 times larger than that of another, then during loss minimization, the gradients associated with the larger-scaled feature will likely be larger. This disproportion can cause the optimization process to overemphasize that feature, even though that feature might not actually be too influential in the prediction, potentially skewing weight updates and adversely affecting the overall training process\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range of features: \n",
      "('x_0', 3.71)\n",
      "('x_1', 1.0)\n",
      "('x_2', 5.0)\n"
     ]
    }
   ],
   "source": [
    "# determine range of each feature\n",
    "\n",
    "# range: max - min\n",
    "\n",
    "features_columns = [col for col in data if col != 'y']\n",
    "feature_ranges = {}\n",
    "for feature in features_columns:\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    feature_ranges[feature] = float(max_val - min_val)\n",
    "\n",
    "print(\"range of features: \")\n",
    "for feature in feature_ranges.items():\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and package selection\n",
    "\n",
    "- Because the target column consists of 0s and 1s, this is likely a binary classification problem (predicting y from x features)\n",
    "    - Use a feedforward neural network\n",
    "- Use  pytorch for defining the model, training, evals\n",
    "- Use the scikit-learn package to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# prepare the data for pytorch\n",
    "# -------------------------\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# separate features and target, converting them to numpy arrays with type float32\n",
    "features_values = data[features_columns].values\n",
    "target_values = data['y'].values \n",
    "\n",
    "# convert the numpy arrays to pytorch tensors\n",
    "tensor_features = torch.Tensor(features_values)\n",
    "tensor_target = torch.Tensor(target_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data (80% train, 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# split data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_values, target_values, test_size=0.2, random_state=42) \n",
    "\n",
    "tensor_x_train = torch.Tensor(x_train)\n",
    "tensor_x_test = torch.Tensor(x_test)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "tensor_y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialize the scaler and fit it only on the training data\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pytorch datasets and dataloaders for the train and test sets\n",
    "\n",
    "TODO:\n",
    "- Why? also know exactly what that code is doing\n",
    "- what is batch size? why 2?\n",
    "- why set shuffle to true for train and false for test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)        \n",
    "test_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network\n",
    "\n",
    "TODO:\n",
    "- how to determine number of hidden layers? and number of neurons they each take? \n",
    "- why relu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of features = number of input neurons\n",
    "\n",
    "        # Input layer (3 features) \n",
    "            # -> Hidden layer1 (10 neurons) \n",
    "            # -> Hidden layer2 (5 neurons) \n",
    "            # -> output (2 neurons, 1 output value)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_features, 10),   # first hidden layer with 10 neurons\n",
    "            nn.ReLU(),                       # activation function RELU: max(0, x) \n",
    "            nn.Linear(10, 5),                # second hidden layer with 5 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),                 # output layer\n",
    "            nn.Sigmoid()                     # final activation for binary classification (probabilities between 0 and 1)\n",
    "        )  \n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- why BCEloss?\n",
    "- why adam optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# intialize the model, loss function, and optimizer\n",
    "\n",
    "input_dim = len(features_columns)\n",
    "model = NeuralNetwork(input_dim)\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train method to train the model & print training accuracy\n",
    "\n",
    "- what is epoch? how to determine its value?\n",
    "- what is batch?\n",
    "- what is .zero_grad()?\n",
    "- determine what EACH line of code is doing in this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'total' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 33\u001b[0m\n\u001b[1;32m     27\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39mtotal\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m train_model(model, train_dataloader, loss_criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, loss_criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_features, batch_labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()                           \u001b[38;5;66;03m# reset gradients\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'total' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, loss_criterion, optimizer, num_epochs=50):\n",
    "    model.train()   # set to training mode\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total += 1\n",
    "        for batch_features, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()                           # reset gradients\n",
    "            y_pred = model(batch_features)                  # forward pass using batch data\n",
    "            loss = loss_criterion(y_pred.squeeze(), batch_labels)  # compute loss using batch data\n",
    "            loss.backward()                                 # backpropagation\n",
    "            optimizer.step()                                # update weights\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        model.eval() # set to eval mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_dataloader:\n",
    "                outputs = model(features)\n",
    "                preds = outputs\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        accuracy = correct /total\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "train_model(model, train_dataloader, loss_criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can this be done better? That improves accuracy? (add those changes as comments)\n",
    "    - bigger scale/more layers/neurons\n",
    "    - dropout... (LEARN WHAT THIS IS)\n",
    "    - \n",
    "\n",
    "- If this was time series how would you take that into account?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x_0  x_1  x_2  y\n",
      "0  1.00    0    0  0\n",
      "1  0.00    0    5  0\n",
      "2  1.00    1    3  1\n",
      "3  0.00    1    1  0\n",
      "4  0.00    1    1  1\n",
      "5  0.00    1    1  0\n",
      "6  3.71    0    1  1\n",
      "7  1.10    0    1  0\n",
      "8  1.00    0    0  1\n",
      "9  1.00    1    1  0\n",
      "data shape (rows, columns): (10, 4)\n",
      "range of features:\n",
      "x_0: 3.71\n",
      "x_1: 1.0\n",
      "x_2: 5.0\n",
      "training initial model...\n",
      "epoch 1/50, average loss: 0.6708, test accuracy: 0.5000\n",
      "epoch 2/50, average loss: 0.6662, test accuracy: 0.5000\n",
      "epoch 3/50, average loss: 0.6603, test accuracy: 0.5000\n",
      "epoch 4/50, average loss: 0.6571, test accuracy: 0.5000\n",
      "epoch 5/50, average loss: 0.6499, test accuracy: 0.5000\n",
      "epoch 6/50, average loss: 0.6445, test accuracy: 0.5000\n",
      "epoch 7/50, average loss: 0.6406, test accuracy: 0.5000\n",
      "epoch 8/50, average loss: 0.6368, test accuracy: 0.5000\n",
      "epoch 9/50, average loss: 0.6305, test accuracy: 0.5000\n",
      "epoch 10/50, average loss: 0.6237, test accuracy: 0.5000\n",
      "epoch 11/50, average loss: 0.6185, test accuracy: 0.5000\n",
      "epoch 12/50, average loss: 0.6096, test accuracy: 0.5000\n",
      "epoch 13/50, average loss: 0.6044, test accuracy: 0.5000\n",
      "epoch 14/50, average loss: 0.5930, test accuracy: 0.5000\n",
      "epoch 15/50, average loss: 0.5865, test accuracy: 0.5000\n",
      "epoch 16/50, average loss: 0.5749, test accuracy: 0.5000\n",
      "epoch 17/50, average loss: 0.5638, test accuracy: 0.5000\n",
      "epoch 18/50, average loss: 0.5525, test accuracy: 0.5000\n",
      "epoch 19/50, average loss: 0.5442, test accuracy: 0.5000\n",
      "epoch 20/50, average loss: 0.5334, test accuracy: 0.5000\n",
      "epoch 21/50, average loss: 0.5244, test accuracy: 0.5000\n",
      "epoch 22/50, average loss: 0.5173, test accuracy: 0.5000\n",
      "epoch 23/50, average loss: 0.5044, test accuracy: 0.5000\n",
      "epoch 24/50, average loss: 0.4941, test accuracy: 0.5000\n",
      "epoch 25/50, average loss: 0.4855, test accuracy: 0.5000\n",
      "epoch 26/50, average loss: 0.4787, test accuracy: 0.5000\n",
      "epoch 27/50, average loss: 0.4834, test accuracy: 0.5000\n",
      "epoch 28/50, average loss: 0.4654, test accuracy: 0.5000\n",
      "epoch 29/50, average loss: 0.4627, test accuracy: 0.5000\n",
      "epoch 30/50, average loss: 0.4582, test accuracy: 0.5000\n",
      "epoch 31/50, average loss: 0.4530, test accuracy: 0.5000\n",
      "epoch 32/50, average loss: 0.4462, test accuracy: 0.5000\n",
      "epoch 33/50, average loss: 0.4434, test accuracy: 0.5000\n",
      "epoch 34/50, average loss: 0.4400, test accuracy: 0.5000\n",
      "epoch 35/50, average loss: 0.4301, test accuracy: 0.5000\n",
      "epoch 36/50, average loss: 0.4270, test accuracy: 0.5000\n",
      "epoch 37/50, average loss: 0.4252, test accuracy: 0.5000\n",
      "epoch 38/50, average loss: 0.4179, test accuracy: 0.5000\n",
      "epoch 39/50, average loss: 0.4148, test accuracy: 0.5000\n",
      "epoch 40/50, average loss: 0.4147, test accuracy: 0.5000\n",
      "epoch 41/50, average loss: 0.4112, test accuracy: 0.5000\n",
      "epoch 42/50, average loss: 0.4065, test accuracy: 0.5000\n",
      "epoch 43/50, average loss: 0.4020, test accuracy: 0.5000\n",
      "epoch 44/50, average loss: 0.3998, test accuracy: 0.5000\n",
      "epoch 45/50, average loss: 0.3984, test accuracy: 0.5000\n",
      "epoch 46/50, average loss: 0.3944, test accuracy: 0.5000\n",
      "epoch 47/50, average loss: 0.3910, test accuracy: 0.5000\n",
      "epoch 48/50, average loss: 0.3885, test accuracy: 0.5000\n",
      "epoch 49/50, average loss: 0.3850, test accuracy: 0.5000\n",
      "epoch 50/50, average loss: 0.3824, test accuracy: 0.5000\n",
      "\n",
      "training improved model...\n",
      "epoch 1/50, average loss: 0.7327, test accuracy: 0.5000\n",
      "epoch 2/50, average loss: 0.7269, test accuracy: 0.5000\n",
      "epoch 3/50, average loss: 0.7143, test accuracy: 0.5000\n",
      "epoch 4/50, average loss: 0.7019, test accuracy: 0.5000\n",
      "epoch 5/50, average loss: 0.7082, test accuracy: 0.5000\n",
      "epoch 6/50, average loss: 0.6638, test accuracy: 0.5000\n",
      "epoch 7/50, average loss: 0.6563, test accuracy: 0.0000\n",
      "epoch 8/50, average loss: 0.6900, test accuracy: 0.0000\n",
      "epoch 9/50, average loss: 0.6068, test accuracy: 0.0000\n",
      "epoch 10/50, average loss: 0.6192, test accuracy: 0.0000\n",
      "epoch 11/50, average loss: 0.6249, test accuracy: 0.0000\n",
      "epoch 12/50, average loss: 0.5951, test accuracy: 0.0000\n",
      "epoch 13/50, average loss: 0.6329, test accuracy: 0.0000\n",
      "epoch 14/50, average loss: 0.5292, test accuracy: 0.0000\n",
      "epoch 15/50, average loss: 0.5452, test accuracy: 0.0000\n",
      "epoch 16/50, average loss: 0.5883, test accuracy: 0.0000\n",
      "epoch 17/50, average loss: 0.5935, test accuracy: 0.0000\n",
      "epoch 18/50, average loss: 0.5203, test accuracy: 0.0000\n",
      "epoch 19/50, average loss: 0.5119, test accuracy: 0.0000\n",
      "epoch 20/50, average loss: 0.7074, test accuracy: 0.0000\n",
      "epoch 21/50, average loss: 0.5192, test accuracy: 0.0000\n",
      "epoch 22/50, average loss: 0.5601, test accuracy: 0.0000\n",
      "epoch 23/50, average loss: 0.5708, test accuracy: 0.0000\n",
      "epoch 24/50, average loss: 0.4116, test accuracy: 0.0000\n",
      "epoch 25/50, average loss: 0.5806, test accuracy: 0.0000\n",
      "epoch 26/50, average loss: 0.4497, test accuracy: 0.0000\n",
      "epoch 27/50, average loss: 0.5599, test accuracy: 0.0000\n",
      "epoch 28/50, average loss: 0.4820, test accuracy: 0.0000\n",
      "epoch 29/50, average loss: 0.5888, test accuracy: 0.0000\n",
      "epoch 30/50, average loss: 0.4374, test accuracy: 0.0000\n",
      "epoch 31/50, average loss: 0.5208, test accuracy: 0.0000\n",
      "epoch 32/50, average loss: 0.4798, test accuracy: 0.0000\n",
      "epoch 33/50, average loss: 0.5310, test accuracy: 0.0000\n",
      "epoch 34/50, average loss: 0.4643, test accuracy: 0.0000\n",
      "epoch 35/50, average loss: 0.3295, test accuracy: 0.0000\n",
      "epoch 36/50, average loss: 0.4192, test accuracy: 0.0000\n",
      "epoch 37/50, average loss: 0.4748, test accuracy: 0.0000\n",
      "epoch 38/50, average loss: 0.5797, test accuracy: 0.0000\n",
      "epoch 39/50, average loss: 0.3896, test accuracy: 0.0000\n",
      "epoch 40/50, average loss: 0.3967, test accuracy: 0.0000\n",
      "epoch 41/50, average loss: 0.4292, test accuracy: 0.0000\n",
      "epoch 42/50, average loss: 0.4917, test accuracy: 0.0000\n",
      "epoch 43/50, average loss: 0.4645, test accuracy: 0.0000\n",
      "epoch 44/50, average loss: 0.4107, test accuracy: 0.0000\n",
      "epoch 45/50, average loss: 0.3367, test accuracy: 0.0000\n",
      "epoch 46/50, average loss: 0.3240, test accuracy: 0.0000\n",
      "epoch 47/50, average loss: 0.4085, test accuracy: 0.0000\n",
      "epoch 48/50, average loss: 0.3094, test accuracy: 0.0000\n",
      "epoch 49/50, average loss: 0.2322, test accuracy: 0.0000\n",
      "epoch 50/50, average loss: 0.1832, test accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# -------------------------\n",
    "# load and inspect the data\n",
    "# -------------------------\n",
    "data_path = 'data.csv'\n",
    "data = pd.read_csv(data_path)  # load csv file into a dataframe\n",
    "\n",
    "# print the dataframe\n",
    "print(data)\n",
    "\n",
    "# print the number of data points (rows) and number of features (columns)\n",
    "print(\"data shape (rows, columns):\", data.shape)\n",
    "\n",
    "# assume that the target column is named 'y'\n",
    "features_columns = [col for col in data.columns if col != 'y']\n",
    "\n",
    "# calculate the range (max - min) for each feature\n",
    "feature_ranges = {}\n",
    "for feature in features_columns:\n",
    "    min_val = data[feature].min()\n",
    "    max_val = data[feature].max()\n",
    "    feature_ranges[feature] = float(max_val - min_val)\n",
    "\n",
    "print(\"range of features:\")\n",
    "for feature, rng in feature_ranges.items():\n",
    "    print(f\"{feature}: {rng}\")\n",
    "\n",
    "# -------------------------\n",
    "# prepare the data for pytorch\n",
    "# -------------------------\n",
    "# separate features and target and convert to numpy arrays\n",
    "features_values = data[features_columns].values.astype(np.float32)\n",
    "target_values = data['y'].values.astype(np.float32)\n",
    "\n",
    "# split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features_values, target_values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# convert numpy arrays to pytorch tensors\n",
    "tensor_x_train = torch.tensor(x_train)\n",
    "tensor_x_test = torch.tensor(x_test)\n",
    "tensor_y_train = torch.tensor(y_train)\n",
    "tensor_y_test = torch.tensor(y_test)\n",
    "\n",
    "# create pytorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "# fix: use test_dataset for the test dataloader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# -------------------------\n",
    "# build the initial neural network model\n",
    "# -------------------------\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        # simple feed-forward network:\n",
    "        # input -> hidden1 (10 neurons) -> hidden2 (5 neurons) -> output (1 neuron)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_features, 10),  # first hidden layer\n",
    "            nn.ReLU(),                      # activation function\n",
    "            nn.Linear(10, 5),               # second hidden layer\n",
    "            nn.ReLU(),                      # activation function\n",
    "            nn.Linear(5, 1),                # output layer\n",
    "            nn.Sigmoid()                    # sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "input_dim = len(features_columns)\n",
    "model = NeuralNetwork(input_dim)\n",
    "\n",
    "# define loss function and optimizer\n",
    "loss_criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# -------------------------\n",
    "# training function that prints loss and accuracy after each epoch\n",
    "# -------------------------\n",
    "def train_model(model, train_loader, test_loader, loss_fn, optimzr, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # training loop\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            optimzr.zero_grad()  # reset gradients\n",
    "            predictions = model(batch_features)  # forward pass\n",
    "            loss = loss_fn(predictions.squeeze(), batch_labels)\n",
    "            loss.backward()  # backpropagation\n",
    "            optimzr.step()   # update weights\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # evaluation on test set for accuracy\n",
    "        model.eval()  # set to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                outputs = model(features)\n",
    "                # convert probabilities to binary predictions\n",
    "                preds = (outputs.squeeze() >= 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        print(f\"epoch {epoch+1}/{num_epochs}, average loss: {avg_loss:.4f}, test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"training initial model...\")\n",
    "train_model(model, train_dataloader, test_dataloader, loss_criterion, optimizer, num_epochs=50)\n",
    "\n",
    "# -------------------------\n",
    "# build an improved model (example: adding dropout to reduce overfitting)\n",
    "# -------------------------\n",
    "class ImprovedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_features, 16),   # increase neurons in first layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),                 # dropout to prevent overfitting\n",
    "            nn.Linear(16, 8),                # hidden layer with dropout effect\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8, 1),                 # output layer\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# initialize improved model, loss function, and optimizer\n",
    "improved_model = ImprovedNeuralNetwork(input_dim)\n",
    "improved_optimizer = optim.Adam(improved_model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\ntraining improved model...\")\n",
    "train_model(improved_model, train_dataloader, test_dataloader, loss_criterion, improved_optimizer, num_epochs=50)\n",
    "\n",
    "# -------------------------\n",
    "# handling time series data\n",
    "# -------------------------\n",
    "# if the data were a time series, we would need to account for the sequential order.\n",
    "# potential approaches include:\n",
    "#\n",
    "# - using recurrent neural networks (rnn, lstm, or gru) to capture temporal dependencies.\n",
    "# - designing the input so that it includes lagged observations (e.g., sliding windows)\n",
    "# - using transformer-based models that are designed for sequential data.\n",
    "#\n",
    "# for example, an lstm-based model might look like:\n",
    "#\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_features, hidden_size, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_features, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # assume x has shape (batch, sequence_length, input_features)\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         # take output of last time step\n",
    "#         last_output = lstm_out[:, -1, :]\n",
    "#         out = self.fc(last_output)\n",
    "#         return self.sigmoid(out)\n",
    "#\n",
    "# the training procedure would then need to handle sequential batches accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6779\n",
      "Epoch 1, Accuracy: 50.00%\n",
      "Epoch 2, Train Loss: 0.6871\n",
      "Epoch 2, Accuracy: 50.00%\n",
      "Epoch 3, Train Loss: 0.6810\n",
      "Epoch 3, Accuracy: 50.00%\n",
      "Epoch 4, Train Loss: 0.6795\n",
      "Epoch 4, Accuracy: 50.00%\n",
      "Epoch 5, Train Loss: 0.6774\n",
      "Epoch 5, Accuracy: 50.00%\n",
      "Epoch 6, Train Loss: 0.6798\n",
      "Epoch 6, Accuracy: 50.00%\n",
      "Epoch 7, Train Loss: 0.6736\n",
      "Epoch 7, Accuracy: 50.00%\n",
      "Epoch 8, Train Loss: 0.6749\n",
      "Epoch 8, Accuracy: 50.00%\n",
      "Epoch 9, Train Loss: 0.6718\n",
      "Epoch 9, Accuracy: 50.00%\n",
      "Epoch 10, Train Loss: 0.6687\n",
      "Epoch 10, Accuracy: 50.00%\n",
      "Epoch 11, Train Loss: 0.6581\n",
      "Epoch 11, Accuracy: 50.00%\n",
      "Epoch 12, Train Loss: 0.6681\n",
      "Epoch 12, Accuracy: 50.00%\n",
      "Epoch 13, Train Loss: 0.6690\n",
      "Epoch 13, Accuracy: 50.00%\n",
      "Epoch 14, Train Loss: 0.6666\n",
      "Epoch 14, Accuracy: 50.00%\n",
      "Epoch 15, Train Loss: 0.6575\n",
      "Epoch 15, Accuracy: 50.00%\n",
      "Epoch 16, Train Loss: 0.6571\n",
      "Epoch 16, Accuracy: 50.00%\n",
      "Epoch 17, Train Loss: 0.6607\n",
      "Epoch 17, Accuracy: 50.00%\n",
      "Epoch 18, Train Loss: 0.6524\n",
      "Epoch 18, Accuracy: 50.00%\n",
      "Epoch 19, Train Loss: 0.6544\n",
      "Epoch 19, Accuracy: 50.00%\n",
      "Epoch 20, Train Loss: 0.6546\n",
      "Epoch 20, Accuracy: 50.00%\n",
      "\n",
      "Final accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load and preprocess\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data.fillna(data.median())\n",
    "X = data.drop('y', axis=1).values.astype(np.float32)\n",
    "y = data['y'].values.astype(np.float32)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize\n",
    "model = NeuralNetwork(X.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch).squeeze()\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            predicted = (y_pred >= 0.5).float()\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "        accuracy = 100 * correct/total\n",
    "        print(f'Epoch {epoch+1}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print(f'\\nFinal accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['y'].value_counts())  # Verify class distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
