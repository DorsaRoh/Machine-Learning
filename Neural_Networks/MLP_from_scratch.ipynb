{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key notes:\n",
    "\n",
    "- a network *\"learns\"* by modifying its weights. Training a neural network means **finding the right **weights and biases** so the network can solve the problem**. I.e. **minimizing a cost function**\n",
    "<br><br>\n",
    "- Think of a neuron as a function, that takes the activations of ALL neurons in the previous layer, and outputs a number greater than 0\n",
    "<br><br>\n",
    "- The activation of a neuron is a measure of how \"positive\" the relevant weighted sum is.\n",
    "<br><br>\n",
    "- The bias is simply a value that lets us choose when a neuron is meaningfully active. Think: \"bias for inactivity\". Ex. only want neurons with a weighted sum > 10 to be activated, set the bias = -10.\n",
    "<br><br>\n",
    "- The weighted sum in a neural network represents the combined influence of all input neurons (i.e., neurons from the previous layer) on a single neuron in the current layer.\n",
    "<br><br>\n",
    "    - It signifies the strength of a connection, determining how much impact each \n",
    "    input has on the neuron's output and its potential to activate based on the \n",
    "    weighted input signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "\n",
    "    def set_activation(self, value: float) -> None:\n",
    "        \"\"\" Set the activation value of the neuron \"\"\"\n",
    "        self.activation = value\n",
    "\n",
    "    def set_bias(self, value: float) -> None:\n",
    "        \"\"\" Set the bias of the neuron \"\"\"\n",
    "        self.bias = value\n",
    "\n",
    "    def get_bias(self) -> None:\n",
    "        \"\"\" Return the bias of the neuron \"\"\"\n",
    "        return self.bias\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # num_inputs : number of neurons in the previous layer\n",
    "    def __init__(self, num_inputs: int, num_neurons: int):\n",
    "        \"\"\" Initialize a layer.\n",
    "            - num_inputs: number of neurons in the PREVIOUS layer.\n",
    "            - num_neurons: number of neurons in the CURRENT layer.\n",
    "        \"\"\"\n",
    "        self.neurons = [Neuron() for _ in range(num_neurons)]       # list of neuron objects\n",
    "        self.weights = np.random.randn(num_inputs, num_neurons)     # matrix of (initially random) values/weights. shape (num_inputs, num_neurons)\n",
    "        self.biases = np.zeros(num_neurons) # bias vector\n",
    "\n",
    "    def set_activations(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\" Calculate the weighted sum for a neuron in the layer \n",
    "            (this is NOT the neuron's activation value). \"\"\"\n",
    "        self.activations = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def apply_activation_function(self) -> None:\n",
    "        \"\"\" Apply the activation function (ReLU) to the weighted sum. \n",
    "            This is the value of the neuron's activation. \"\"\"\n",
    "        self.activations = self.relu(self.activations)\n",
    "\n",
    "    def get_activations(self) -> np.ndarray:\n",
    "        \"\"\" Return the activations of the layer. \"\"\"\n",
    "        return self.activations\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" \n",
    "        Apply the rely activation function. \n",
    "        If the input value > 0, return the input value.\n",
    "        If the input value == 0, return 0.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_layer_forward_pass (__main__.TestLayer.test_layer_forward_pass) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestLayer(unittest.TestCase):\n",
    "    def test_layer_forward_pass(self):\n",
    "        num_inputs = 3  # Number of neurons in the previous layer\n",
    "        num_neurons = 2  # Number of neurons in the current layer\n",
    "        layer = Layer(num_inputs, num_neurons)\n",
    "        \n",
    "        inputs = np.array([0.5, 0.3, 0.9])  # Example input activations\n",
    "        layer.set_activations(inputs)\n",
    "        layer.apply_activation_function()\n",
    "        activations = layer.get_activations()\n",
    "        \n",
    "        self.assertEqual(activations.shape, (num_neurons,))  # Ensure correct shape\n",
    "        self.assertTrue((activations >= 0).all())  # Ensure ReLU outputs are non-negative\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the tests with detailed output\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestLayer)\n",
    "    unittest.TextTestRunner(verbosity=2).run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: gradient descent & loss functions from scratch. everything up until backprop\n",
    "\n",
    "# loss function: telling the computer that its output is bad.\n",
    "\n",
    "\n",
    "def calculate_loss(expected: np.ndarray[float], actual: np.ndarray[float]) -> float: \n",
    "    # len(expected) == len(actual)\n",
    "    losses = []\n",
    "    for i in range(len(expected)):\n",
    "        losses.append(abs(actual[i] - expected[i]))\n",
    "    return sum(losses)\n",
    "\n",
    "def calculate_average_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
